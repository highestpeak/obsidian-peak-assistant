import { LLMStreamEvent, LLMUsage, LLMRequestMessage, StreamTriggerName, ToolResultOutput } from '@/core/providers/types';
import { toAiSdkMessages } from '@/core/providers/adapter/ai-sdk-adapter';
import { contentReaderTool } from '@/service/tools/content-reader';
import { vaultGraphInspectorTool } from '@/service/tools/search-graph-inspector';
import { localWebSearchTool } from '@/service/tools/search-web';
import { Experimental_Agent as Agent, hasToolCall, stepCountIs } from 'ai';
import { genSystemInfo } from '@/service/tools/system-info';
import { PromptId } from '@/service/prompt/PromptId';
import { submitFinalAnswerTool } from '@/service/tools/submit-final-answer';
import { AgentTool, safeAgentTool } from '@/service/tools/types';
import { z } from 'zod/v3';
import { buildLLMRequestMessage, concatLLMRequestMessages } from '@/core/providers/helpers/message-helper';
import { AIServiceManager } from '../chat/service-manager';

export interface AISearchAgentOptions {
    enableWebSearch?: boolean;
    enableLocalSearch?: boolean;
    /**
     * Maximum iterations for multi-agent ReAct loop.
     * Default: 10
     * come from setting config.
     */
    maxMultiAgentIterations?: number;
    /**
     * model id for thought agent
     */
    thoughtAgentModel: string;
    /**
     * provider for thought agent
     */
    thoughtAgentProvider: string;
    /**
     * model id for search agent
     */
    searchAgentModel: string;
    /**
     * provider for search agent
     */
    searchAgentProvider: string;
}

/**
 * Tool set for thought agent (coordinator)
 */
type ThoughtToolSet = {
    call_search_agent: AgentTool;
    update_result: AgentTool;
    submit_final_answer: AgentTool;
};

/**
 * Tool set for search agent (executor)
 */
type SearchToolSet = {
    content_reader: AgentTool;
    web_search?: AgentTool;
    vault_inspector?: AgentTool;
    submit_final_answer: AgentTool;
};

const DEFAULT_MAX_RECENT_MESSAGES = 10;
const DEFAULT_SUMMARY_UPDATE_THRESHOLD = 5;

export interface InsightCard {
    id: string;
    title: string;
    description: string;
    icon: string;
    color: string;
}

export interface Suggestion {
    id: string;
    title: string;
    description: string;
    icon: string;
    color: string;
}

export interface AISearchSource {
    id: string;
    title: string;
    // important: we will open file by this path.
    path: string;
    // why it was selected or rejected.
    reasoning: string;
    // add badges to the item to quickly judge the role of each note in the current analysis.
    badges: string[];
    score: {
        // 0~100
        physical: number;
        semantic: number;
        average: number;
    }
}

export interface AISearchNode {
    // uuid of the node. generated by agent.
    id: string;
    // we can add many types of nodes. like file, folder, concept, tag, etc. determined by Agent.
    type: string;
    // title of the node. generated by agent.
    title: string;
    // important: we will open file by this path.
    path?: string;
    // attributes of the node. like tags, categories, etc. determined by Agent.
    attributes: {
        [key: string]: any;
    };
}

export interface AISearchEdge {
    // uuid of the edge. generated by agent.
    id: string;
    // uuid of the source node.
    source: string;
    // type of the edge. like link, reference, etc. determined by Agent.
    type: string;
    // uuid of the target node.
    target: string;
    // attributes of the edge. like weight, etc. determined by Agent.
    attributes: {
        [key: string]: any;
    };
}

export interface AISearchTopic {
    label: string;
    weight: number
}

export interface AISearchGraph {
    nodes: AISearchNode[];
    edges: AISearchEdge[];
}

export interface SearchAgentResult {
    summary: string;
    graph: AISearchGraph;
    insightCards?: InsightCard[];
    suggestions?: Suggestion[];
    topics: AISearchTopic[];
    sources: AISearchSource[];
}

export interface AgentMemory {
    /**
     * the original prompt from user
     */
    initialPrompt: string;
    /**
     * all messages in the session
     * messages includes tool calls and results.
     * actually. these will include the discovered_key_nodes, rejected_nodes, etc. so we don't need to store them separately.
     */
    historyMessages: LLMRequestMessage[];
    /**
     * summary of the session for 0~n messages due to the context window of the model
     */
    sessionSummary: string;
    /**
     * index of the last summary
     */
    lastSummaryIndex: number;
    /**
     * latest messages in the session
     */
    latestMessages: LLMRequestMessage[];
    /**
     * current query from user and assistant intermediate calls
     */
    currentQuery: string;
    /**
     * total token usage for the session
     */
    totalTokenUsage: LLMUsage;
}

/**
 * Search Agent.
 * ReAct architecture.
 * Multi agent architecture. (SubAgents)
 */
export class AISearchAgent {
    /**
     * Thought Agent - main coordinator for ReAct loop
     */
    private thoughtAgent: Agent<ThoughtToolSet>;

    /**
     * Search Agent - sub agent for search tasks
     */
    private searchAgent: Agent<SearchToolSet>;

    /**
     * Maximum iterations for multi-agent ReAct loop
     */
    private maxIterations: number;
    /**
     * Agent memory
     */
    private agentMemory: AgentMemory;
    /**
     * Agent result
     */
    private agentResult: SearchAgentResult;

    constructor(
        private readonly aiServiceManager: AIServiceManager,
        private options: AISearchAgentOptions,
    ) {
        this.maxIterations = this.options.maxMultiAgentIterations ?? 10;
        // Create search agent (focused on search tasks, no submit_final_answer)
        let searchTools: SearchToolSet = {
            content_reader: contentReaderTool(),
            submit_final_answer: submitFinalAnswerTool(),
        };
        // todo
        // if (this.options.enableWebSearch) {
        //     searchTools.web_search = localWebSearchTool();
        // }
        if (this.options.enableLocalSearch) {
            searchTools.vault_inspector = vaultGraphInspectorTool();
        }
        this.searchAgent = new Agent<SearchToolSet>({
            model: this.aiServiceManager.getMultiChat()
                .getProviderService(this.options.searchAgentProvider)
                .modelClient(this.options.searchAgentModel),
            tools: searchTools,
            stopWhen: [
                stepCountIs(10),
                // stop when the submit_final_answer tool is called
                hasToolCall('submit_final_answer'),
            ],
        });

        // Create thought agent (main coordinator)
        const thoughtTools: ThoughtToolSet = {
            call_search_agent: this.callSearchAgentTool(),
            update_result: this.updateAgentResultTool(),
            submit_final_answer: submitFinalAnswerTool(),
        };
        this.thoughtAgent = new Agent<ThoughtToolSet>({
            model: this.aiServiceManager.getMultiChat()
                .getProviderService(this.options.thoughtAgentProvider)
                .modelClient(this.options.thoughtAgentModel),
            tools: thoughtTools,
            stopWhen: [
                // default: stop at every call. we manually control the loop using ReAct loop.
                stepCountIs(1),
                // stop when the submit_final_answer tool is called
                hasToolCall('submit_final_answer'),
            ],
            // do not use tool choice. this only control the next tool to be called.
            // don't understand this too much currently. don't find enough documentation about it.
            // toolChoice: {
            //     type: 'tool',
            //     toolName: 'submit_final_answer',
            // },
        });
    }

    /**
     * Stream search execution (used internally by thought agent)
     */
    private async streamSearch(prompt: string): Promise<AsyncGenerator<LLMStreamEvent>> {
        if (!prompt) {
            return (async function* (): AsyncGenerator<LLMStreamEvent> {
                yield { type: 'error', error: new Error('search prompt is required') };
            })();
        }

        const system = await this.aiServiceManager.renderPrompt(
            PromptId.AiSearchSystem,
            await genSystemInfo()
        );
        console.debug('[AISearchAgent] searchAgent streamSearch system:', system);
        console.debug('[AISearchAgent] searchAgent streamSearch prompt:', prompt);
        // read and learn: https://gist.github.com/sshh12/25ad2e40529b269a88b80e7cf1c38084
        const result = this.searchAgent.stream({
            system: system,
            prompt,
        });

        return (async function* (): AsyncGenerator<LLMStreamEvent> {
            let finalSummary: string = '';
            const reasoningTextChunks: string[] = [];
            const thoughtTextChunks: string[] = [];
            for await (const chunk of result.fullStream) {
                // // debug log for all chunks.
                // // text and reasoning are too much to log. useless. only log the type to observe the flow.
                // if (chunk.type !== 'text-delta' && chunk.type !== 'reasoning-delta') {
                //     console.debug('[AISearchAgent] streamSearch stream chunk:', JSON.stringify(chunk));
                // }
                switch (chunk.type) {
                    case 'text-delta':
                        thoughtTextChunks.push(chunk.text);
                        yield { type: 'text-delta', text: chunk.text, triggerName: StreamTriggerName.SEARCH_INSPECTOR_AGENT };
                        break;
                    case 'reasoning-delta':
                        reasoningTextChunks.push(chunk.text);
                        yield { type: 'reasoning-delta', text: chunk.text, triggerName: StreamTriggerName.SEARCH_INSPECTOR_AGENT };
                        break;
                    case 'tool-call':
                        if (chunk.toolName === 'submit_final_answer') {
                            finalSummary = chunk.input.summary;
                            break;
                        }
                        yield { type: 'tool-call', toolName: chunk.toolName, input: chunk.input, triggerName: StreamTriggerName.SEARCH_INSPECTOR_AGENT };
                        break;
                    case 'tool-result':
                        yield { type: 'tool-result', toolName: chunk.toolName, input: chunk.input, output: chunk.output, triggerName: StreamTriggerName.SEARCH_INSPECTOR_AGENT };
                        break;
                    case 'finish': {
                        // safeProcess('finalSummary', finalSummary);
                        // safeProcess('thoughtTextChunks', thoughtTextChunks);
                        // safeProcess('reasoningTextChunks', reasoningTextChunks);
                        console.debug('[AISearchAgent] streamSearch complete:', JSON.stringify({
                            summary: finalSummary,
                            text: thoughtTextChunks.join('').trim(),
                            reasoning: reasoningTextChunks.join('').trim(),
                        }));
                        yield {
                            type: 'complete',
                            finishReason: chunk.finishReason,
                            usage: chunk.totalUsage,
                            triggerName: StreamTriggerName.SEARCH_INSPECTOR_AGENT,
                            result: {
                                summary: finalSummary,
                                text: thoughtTextChunks.join('').trim(),
                                reasoning: reasoningTextChunks.join('').trim(),
                            },
                        };
                        break;
                    }
                    case 'start':
                    case 'start-step':
                    case 'reasoning-start':
                    case 'reasoning-end':
                    case 'text-start':
                    case 'text-end':
                    case 'finish-step':
                    case 'tool-input-end':
                    case 'tool-input-start':
                    case 'tool-input-delta':
                        // devtools will merge these duplicate logs.
                        console.debug('[AISearchAgent] streamSearch skip. one of the following types: '
                            + 'start, start-step, reasoning-start, reasoning-end, text-start, text-end, '
                            + 'finish-step, tool-input-start, tool-input-delta, tool-input-end');
                        break;
                    default:
                        yield { type: 'unSupported', chunk: chunk, comeFrom: 'streamSearch', triggerName: StreamTriggerName.SEARCH_INSPECTOR_AGENT };
                        break;
                }
            }
        })();
    }

    /**
     * Execute the ReAct loop (ThoughtAgent coordinates SearchAgent)
     */
    private async *executeReActLoop(initialPrompt: string): AsyncGenerator<LLMStreamEvent> {
        // Initialize agent memory for this session
        this.agentMemory = {
            initialPrompt,
            sessionSummary: '',
            historyMessages: [buildLLMRequestMessage('user', initialPrompt)],
            latestMessages: [buildLLMRequestMessage('user', initialPrompt)],
            currentQuery: initialPrompt,
            lastSummaryIndex: 0,
            totalTokenUsage: {
                inputTokens: 0,
                outputTokens: 0,
                totalTokens: 0,
            }
        };

        let iterationCount = 0;
        let reActStartTimeMs = Date.now();
        while (iterationCount < this.maxIterations) {
            iterationCount++;

            const thoughtTextChunks: string[] = [];
            const reasoningTextChunks: string[] = [];
            let stepTokenUsage: LLMUsage = {
                inputTokens: 0,
                outputTokens: 0,
                totalTokens: 0,
            };
            let toolCalls: Array<{ toolName: string; input: any }> = [];
            let toolResults: Array<{ toolName: string; output: ToolResultOutput }> = [];
            let isSubmitResultCalled = false;

            // Build current prompt with agent memory, yielding progress events
            const promptGenerator = this.buildCurrentPrompt();
            for await (const chunk of promptGenerator) {
                // Yield summary generation progress
                yield chunk;
            }
            // Get the final prompt after summarization is complete
            const currentPrompt = this.agentMemoryToPrompt();
            const nextThoughtPrompt = toAiSdkMessages(currentPrompt)
            console.debug('[AISearchAgent] nextThoughtPrompt:', JSON.stringify(nextThoughtPrompt));

            // Phase 1: ThoughtAgent thinks and decides next action (streaming)
            const thoughtStream = this.thoughtAgent.stream({
                system: await this.aiServiceManager.renderPrompt(PromptId.ThoughtAgentSystem, {}),
                prompt: nextThoughtPrompt,
            });

            // Process thoughtAgent's stream in real-time
            for await (const chunk of thoughtStream.fullStream) {
                // // debug log for all chunks.
                // // text and reasoning are too much to log. useless. only log the type to observe the flow.
                // if (chunk.type !== 'text-delta' && chunk.type !== 'reasoning-delta') {
                //     console.debug('[AISearchAgent] thoughtAgent stream chunk:', JSON.stringify(chunk));
                // }
                // console.debug('[AISearchAgent] thoughtAgent stream chunk:', chunk.type);
                switch (chunk.type) {
                    case 'text-delta':
                        thoughtTextChunks.push(chunk.text);
                        yield { type: 'text-delta', text: chunk.text, triggerName: StreamTriggerName.SEARCH_THOUGHT_AGENT };
                        break;
                    case 'reasoning-delta':
                        reasoningTextChunks.push(chunk.text);
                        yield { type: 'reasoning-delta', text: chunk.text, triggerName: StreamTriggerName.SEARCH_THOUGHT_AGENT };
                        break;
                    case 'tool-call':
                        toolCalls.push({ toolName: chunk.toolName, input: chunk.input });
                        if (chunk.toolName === 'submit_final_answer') {
                            isSubmitResultCalled = true;
                        }
                        yield {
                            type: 'tool-call',
                            id: chunk.toolCallId,
                            toolName: chunk.toolName,
                            input: chunk.input,
                            triggerName: StreamTriggerName.SEARCH_THOUGHT_AGENT,
                        };

                        // Handle search agent call immediately to get streaming output
                        if (chunk.toolName === 'call_search_agent') {
                            const { prompt: searchPrompt } = chunk.input;
                            const searchStream = await this.streamSearch(searchPrompt);

                            // Forward search agent output in real-time
                            const searchResultChunks: Record<string, any> = {};
                            for await (const searchChunk of searchStream) {
                                switch (searchChunk.type) {
                                    case 'complete':
                                        stepTokenUsage = this.mergeTokenUsage(stepTokenUsage, searchChunk.usage);
                                        // complete event will be handled by the thoughtAgent stream
                                        // only process final summary. we don't need to keep the other parts.
                                        searchResultChunks.summary = searchChunk.result?.summary.trim().length > 0
                                            // if summary is empty, use text instead. (but i don't know why it happens currently. i will fix this later if i have time.)
                                            ? searchChunk.result?.summary
                                            : searchChunk.result?.text;
                                        // searchResultChunks.text = searchChunk.result?.text;
                                        // searchResultChunks.reasoning = searchChunk.result?.reasoning;
                                        break;
                                    default:
                                        yield searchChunk;
                                        break;
                                }
                            }

                            // Yield tool result for the search call
                            toolResults.push({
                                toolName: 'call_search_agent',
                                output: {
                                    type: 'text',
                                    value: JSON.stringify(searchResultChunks),
                                }
                            });
                            yield {
                                type: 'tool-result',
                                id: chunk.toolCallId,
                                toolName: 'call_search_agent',
                                input: chunk.input,
                                output: searchResultChunks,
                                triggerName: StreamTriggerName.SEARCH_THOUGHT_AGENT
                            };
                        }
                        break;
                    case 'tool-result':
                        if (chunk.toolName !== 'call_search_agent') {
                            toolResults.push({
                                toolName: chunk.toolName,
                                output: {
                                    type: 'text',
                                    value: JSON.stringify(chunk.output),
                                }
                            });
                            yield {
                                type: 'tool-result',
                                id: chunk.toolCallId,
                                toolName: chunk.toolName,
                                input: chunk.input,
                                output: chunk.output,
                                triggerName: StreamTriggerName.SEARCH_THOUGHT_AGENT,
                                extra: {
                                    currentResult: this.agentResult,
                                },
                            };
                        }
                        break;
                    case 'finish':
                        stepTokenUsage = this.mergeTokenUsage(stepTokenUsage, chunk.totalUsage);
                        yield {
                            type: 'on-step-finish',
                            text: thoughtTextChunks.join('').trim(),
                            finishReason: chunk.finishReason,
                            usage: chunk.totalUsage,
                            triggerName: StreamTriggerName.SEARCH_THOUGHT_AGENT
                        };
                        break;
                    case 'start':
                    case 'start-step':
                    case 'reasoning-start':
                    case 'reasoning-end':
                    case 'text-start':
                    case 'text-end':
                    case 'finish-step':
                    case 'tool-input-start':
                    case 'tool-input-delta':
                    case 'tool-input-end':
                        // devtools will merge these duplicate logs.
                        console.debug('[AISearchAgent] thoughtAgent skip. one of the following types: '
                            + 'start, start-step, reasoning-start, reasoning-end, text-start, text-end, '
                            + 'finish-step, tool-input-start, tool-input-delta, tool-input-end');
                        break;
                    default:
                        yield { type: 'unSupported', chunk: chunk, comeFrom: 'thoughtAgent', triggerName: StreamTriggerName.SEARCH_THOUGHT_AGENT };
                        break;
                }
            }

            // Record thoughtAgent's response in agent memory
            // safeProcess('thoughtTextChunks', thoughtTextChunks);
            // safeProcess('reasoningTextChunks', reasoningTextChunks);
            const thoughtText = thoughtTextChunks.join('');
            const reasoningText = reasoningTextChunks.join('');
            const thoughtMessage: LLMRequestMessage = {
                role: 'assistant',
                content: []
            }
            if (thoughtText.trim().length > 0) {
                thoughtMessage.content.push({ type: 'text', text: thoughtText.trim() });
            }
            if (reasoningText.trim().length > 0) {
                thoughtMessage.content.push({ type: 'reasoning', text: reasoningText.trim() });
            }
            if (toolCalls.length > 0) {
                thoughtMessage.content.push(
                    ...toolCalls.map(({ toolName, input }) => ({
                        type: 'tool-call' as const,
                        toolName,
                        input
                    }))
                );
            }
            if (toolResults.length > 0) {
                thoughtMessage.content.push(
                    ...toolResults.map(({ toolName, output }) => ({
                        type: 'tool-result' as const,
                        toolCallId: 'unknown',
                        toolName,
                        output
                    }))
                );
            }
            this.agentMemory.historyMessages.push(thoughtMessage);
            this.agentMemory.latestMessages.push(thoughtMessage);
            this.agentMemory.totalTokenUsage = this.mergeTokenUsage(this.agentMemory.totalTokenUsage, stepTokenUsage);

            // Check if final answer was submitted
            if (isSubmitResultCalled) {
                // ThoughtAgent decided to submit final answer, end the loop
                console.debug('ThoughtAgent decided to submit final answer, end the loop');
                // we will stream summary separately and then complete it later
                const summaryStream = this.aiServiceManager.chatWithPromptStream(
                    PromptId.SearchAiSummary,
                    {
                        agentResult: this.agentResult,
                        agentMemory: this.agentMemory,
                        options: this.options,
                    },
                );
                for await (const chunk of summaryStream) {
                    console.debug('[AISearchAgent] summaryStream chunk:', JSON.stringify(chunk));
                    if (chunk.type === 'prompt-stream-result') {
                        this.agentResult.summary = chunk.output;
                    }
                    yield { ...chunk, triggerName: StreamTriggerName.SEARCH_THOUGHT_AGENT };
                }
                console.debug('[AISearchAgent] summaryStream completed');
                yield {
                    type: 'complete',
                    finishReason: 'stop',
                    usage: this.agentMemory.totalTokenUsage,
                    durationMs: Date.now() - reActStartTimeMs,
                    result: this.agentResult,
                    triggerName: StreamTriggerName.SEARCH_THOUGHT_AGENT,
                };
                break;
            }

            // Continue to next thinking round (history will be included in next prompt)
        }
    }

    private mergeTokenUsage(usage1: LLMUsage, usage2: LLMUsage): LLMUsage {
        return {
            inputTokens: (usage1.inputTokens ?? 0) + (usage2.inputTokens ?? 0) || undefined,
            outputTokens: (usage1.outputTokens ?? 0) + (usage2.outputTokens ?? 0) || undefined,
            totalTokens: (usage1.totalTokens ?? 0) + (usage2.totalTokens ?? 0) || undefined,
            reasoningTokens: (usage1.reasoningTokens ?? 0) + (usage2.reasoningTokens ?? 0) || undefined,
            cachedInputTokens: (usage1.cachedInputTokens ?? 0) + (usage2.cachedInputTokens ?? 0) || undefined,
        };
    }

    /**
     * Build current prompt with agent memory, yielding progress events during summarization
     */
    private async *buildCurrentPrompt(): AsyncGenerator<LLMStreamEvent> {
        const history = this.agentMemory.historyMessages;

        // If history is short, include all
        if (history.length <= DEFAULT_SUMMARY_UPDATE_THRESHOLD) {
            return;
        }

        // todo we must summarize according to the window size. not just the length of the history messages.
        //   for conversation also we should do this.
        // Simple heuristic: if we have more than 5 messages, consider summarizing
        if (history.length - this.agentMemory.lastSummaryIndex <= DEFAULT_SUMMARY_UPDATE_THRESHOLD) {
            return;
        }

        // If history is long, do summarization
        yield { type: 'tool-call', toolName: 'summary_context_messages', triggerName: StreamTriggerName.SEARCH_THOUGHT_AGENT };
        const messageNeedToSummary = history.slice(0, -DEFAULT_SUMMARY_UPDATE_THRESHOLD);
        console.debug('[AISearchAgent] Generating summary for history:', messageNeedToSummary.length);
        // Generate summary immediately
        this.agentMemory.sessionSummary = await this.aiServiceManager.chatWithPrompt(PromptId.DocSummary, {
            content: concatLLMRequestMessages(messageNeedToSummary),
            title: `Thought Agent History of the user query: \`${this.agentMemory.initialPrompt}\` `,
            wordCount: 'less than 1000',
        });
        this.agentMemory.lastSummaryIndex = messageNeedToSummary.length - 1;
        this.agentMemory.latestMessages = DEFAULT_MAX_RECENT_MESSAGES > this.agentMemory.historyMessages.length
            ? this.agentMemory.historyMessages
            : [...this.agentMemory.historyMessages.slice(-DEFAULT_MAX_RECENT_MESSAGES),];
        yield { type: 'tool-result', toolName: 'summary_context_messages', triggerName: StreamTriggerName.SEARCH_THOUGHT_AGENT };
    }

    /**
     * Convert agent memory to LLMRequestMessage array
     */
    private agentMemoryToPrompt(): LLMRequestMessage[] {
        if (this.agentMemory.sessionSummary && this.agentMemory.sessionSummary.trim().length > 0) {
            return [
                buildLLMRequestMessage('assistant', this.agentMemory.sessionSummary),
                ...this.agentMemory.latestMessages,
            ];
        }
        return this.agentMemory.latestMessages;
    }

    /**
     * Stream search results with ReAct loop (ThoughtAgent coordinates SearchAgent)
     */
    async stream(prompt: string): Promise<AsyncGenerator<LLMStreamEvent>> {
        this.resetAgentResult();
        return this.executeReActLoop(prompt);
    }

    private resetAgentResult(): SearchAgentResult {
        this.agentResult = {
            summary: '',
            topics: [],
            graph: { nodes: [], edges: [] },
            sources: [],
            insightCards: [],
            suggestions: [],
        };
        return this.agentResult;
    }

    /**
     * Tool for thought agent to call search agent
     */
    private callSearchAgentTool(): AgentTool {
        return safeAgentTool({
            description: "Execute a search task using the search agent. Provide a specific search prompt that focuses on gathering relevant information.",
            inputSchema: z.object({
                prompt: z.string().describe("The search prompt for the search agent"),
            }),
            execute: async ({ prompt }) => {
                // This tool definition is used by thought agent
                // Actual execution is handled in the stream method
                return { prompt };
            },
        });
    }

    /**
     * Update agent result with flexible operations
     */
    private updateAgentResultTool(): AgentTool {
        // Define precise schemas for SearchAgentResult structure
        const graphNodeSchema = z.object({
            id: z.string(),
            type: z.string(),
            title: z.string(),
            path: z.string().optional(),
            attributes: z.record(z.any()),
        });

        const graphEdgeSchema = z.object({
            id: z.string(),
            source: z.string(),
            type: z.string(),
            target: z.string(),
            attributes: z.record(z.any()),
        });

        const sourceSchema = z.object({
            id: z.string(),
            title: z.string(),
            path: z.string(),
            reasoning: z.string(),
            badges: z.array(z.string()),
            score: z.object({
                physical: z.number().min(0).max(100),
                semantic: z.number().min(0).max(100),
                average: z.number().min(0).max(100),
            }),
        });

        const insightCardSchema = z.object({
            id: z.string(),
            title: z.string(),
            description: z.string(),
            icon: z.string(),
            color: z.string(),
        });

        const suggestionSchema = z.object({
            id: z.string(),
            title: z.string(),
            description: z.string(),
            icon: z.string(),
            color: z.string(),
        });

        const topicSchema = z.object({
            label: z.string(),
            weight: z.number(),
        });

        return safeAgentTool({
            description: `Update agent result: add items to arrays or remove items by id.

Available result fields you can update:
- topics: Array of key topics found during the search
- sources: Array of source documents with metadata, scoring, and reasoning
- graph.nodes: Knowledge graph nodes (files, concepts, tags, etc.)
- graph.edges: Relationships between nodes (links, references, etc.)
- insightCards: Actionable insights with icons and colors for quick recognition
- suggestions: Recommended next steps or related actions

Try to generate insightCards and suggestions whenever possible - they help users discover patterns and take action. For example:
- insightCards: "Key themes identified", "Missing connections found", "Knowledge gaps discovered"
- suggestions: "Explore related topics", "Review outdated sources"`,
            inputSchema: z.object({
                operation: z.enum(['add', 'remove']),
                // For add: specify target array field and provide single item to add
                targetField: z.enum(['topics', 'sources', 'graph.nodes', 'graph.edges', 'insightCards', 'suggestions']).optional(),
                item: z.union([
                    topicSchema, // for topics
                    sourceSchema, // for sources
                    graphNodeSchema, // for graph.nodes
                    graphEdgeSchema, // for graph.edges
                    insightCardSchema, // for insightCards
                    suggestionSchema, // for suggestions
                ]).optional(),
                // For remove: specify target array field and provide id to remove
                removeId: z.string().optional(),
            }),
            execute: async (input) => {
                const { operation } = input;

                switch (operation) {
                    case 'add':
                        if (!input.targetField || input.item === undefined) {
                            return 'failed to add item, targetField and item are required';
                        }
                        return this.addItem(input.targetField, input.item);

                    case 'remove':
                        if (!input.targetField || !input.removeId) {
                            return 'failed to remove item, targetField and removeId are required';
                        }
                        return this.removeItem(input.targetField, input.removeId);

                    default:
                        return `failed to update agent result, unknown operation: ${operation}`;
                }
            },
        });
    }

    /**
     * Add an item to the specified array field or set value for non-array fields
     */
    private addItem(targetField: string, item: any) {
        const arrayPath = targetField.split('.');
        let current: any = this.agentResult;

        // Navigate to the field
        for (let i = 0; i < arrayPath.length - 1; i++) {
            const key = arrayPath[i];
            if (!(key in current)) {
                current[key] = {};
            }
            current = current[key];
        }

        const finalKey = arrayPath[arrayPath.length - 1];

        // Handle array fields
        if (!(finalKey in current)) {
            current[finalKey] = [];
        }
        const targetArray = current[finalKey];
        targetArray.push(item);

        return 'successfully updated agent result';
    }

    /**
     * Remove an item from the specified array field by id
     */
    private removeItem(targetField: string, removeId: string) {
        const arrayPath = targetField.split('.');
        let current: any = this.agentResult;

        // Navigate to the array
        for (const key of arrayPath) {
            if (!(key in current)) {
                return `failed to remove item ${removeId} from ${targetField}, field ${targetField} does not exist`;
            }
            current = current[key];
        }

        const targetArray = current;
        const index = targetArray.findIndex((item: any) => item.id === removeId);

        if (index === -1) {
            return `failed to remove item ${removeId} from ${targetField}, item not found`;
        }

        targetArray.splice(index, 1);
        return `successfully removed item ${removeId} from ${targetField}`;
    }
}